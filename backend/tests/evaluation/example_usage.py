#!/usr/bin/env python3
"""
Example usage of the Form Evaluation Framework

This script demonstrates how to use the evaluator programmatically.
"""
import json
from pathlib import Path
from evaluator import FormEvaluator

def example_single_form_evaluation():
    """Example: Evaluate a single generated form against ground truth."""
    print("="*70)
    print("Example: Single Form Evaluation")
    print("="*70)

    # Create evaluator with 80% similarity threshold
    evaluator = FormEvaluator(similarity_threshold=0.8)

    # Example generated schema (would come from your AI system)
    generated_schema = {
        "title": "Contact Form",
        "description": "Get in touch with us",
        "components": [
            {
                "id": "comp_1",
                "type": "short-answer",
                "data": {
                    "question": "What is your name?",
                    "required": True
                }
            },
            {
                "id": "comp_2",
                "type": "short-answer",
                "data": {
                    "question": "What is your email?",
                    "required": True
                }
            },
            {
                "id": "comp_3",
                "type": "multiple-choice",
                "data": {
                    "question": "How did you hear about us?",
                    "options": ["Social Media", "Search", "Friend", "Other"],
                    "required": False
                }
            }
        ]
    }

    # Example ground truth
    ground_truth_schema = {
        "title": "Contact Information Form",
        "description": "Please provide your contact details",
        "components": [
            {
                "id": "gt_1",
                "type": "short-answer",
                "data": {
                    "question": "What is your full name?",
                    "required": True
                }
            },
            {
                "id": "gt_2",
                "type": "short-answer",
                "data": {
                    "question": "What is your email address?",
                    "required": True
                }
            },
            {
                "id": "gt_3",
                "type": "short-answer",
                "data": {
                    "question": "What is your phone number?",
                    "required": False
                }
            },
            {
                "id": "gt_4",
                "type": "multiple-choice",
                "data": {
                    "question": "How did you hear about us?",
                    "options": ["Social Media", "Search Engine", "Friend", "Advertisement"],
                    "required": False
                }
            }
        ]
    }

    # Evaluate
    result = evaluator.evaluate_form(
        generated_schema=generated_schema,
        ground_truth_schema=ground_truth_schema,
        pdf_name="example_contact_form"
    )

    # Print detailed report
    evaluator.print_evaluation_report(result)

    return result


def example_dataset_evaluation():
    """Example: Evaluate multiple forms."""
    print("\n" + "="*70)
    print("Example: Dataset Evaluation")
    print("="*70)

    # Create evaluator
    evaluator = FormEvaluator(similarity_threshold=0.8)

    # Assuming you have:
    # - ground_truth/*.json files
    # - test_forms/*.json files (generated by your system)
    
    ground_truth_dir = Path("ground_truth")
    test_forms_dir = Path("test_forms")

    # Create directories if they don't exist
    ground_truth_dir.mkdir(exist_ok=True)
    test_forms_dir.mkdir(exist_ok=True)

    # Evaluate dataset
    if list(ground_truth_dir.glob("*.json")):
        aggregate_result = evaluator.evaluate_dataset(
            ground_truth_dir=ground_truth_dir,
            generated_forms_dir=test_forms_dir
        )

        # Print aggregate report
        evaluator.print_aggregate_report(aggregate_result)

        # Export results
        output_path = Path("results/evaluation_example.json")
        output_path.parent.mkdir(exist_ok=True)
        evaluator.export_results_to_json(aggregate_result, output_path)

        return aggregate_result
    else:
        print("\nNo ground truth files found in ground_truth/")
        print("Create some JSON files there first!")
        return None


def example_custom_similarity():
    """Example: Using custom similarity threshold."""
    print("\n" + "="*70)
    print("Example: Custom Similarity Threshold")
    print("="*70)

    # Stricter matching (90% similarity required)
    strict_evaluator = FormEvaluator(similarity_threshold=0.9)
    
    # Looser matching (70% similarity required)
    loose_evaluator = FormEvaluator(similarity_threshold=0.7)

    # Example schemas
    generated = {
        "components": [{
            "id": "c1",
            "type": "short-answer",
            "data": {"question": "What's your name?", "required": True}
        }]
    }

    ground_truth = {
        "components": [{
            "id": "gt1",
            "type": "short-answer",
            "data": {"question": "What is your full name?", "required": True}
        }]
    }

    # Evaluate with strict threshold
    strict_result = strict_evaluator.evaluate_form(generated, ground_truth, "strict_test")
    print(f"\nStrict (90%) - Correct: {strict_result.correctly_identified}")

    # Evaluate with loose threshold
    loose_result = loose_evaluator.evaluate_form(generated, ground_truth, "loose_test")
    print(f"Loose (70%) - Correct: {loose_result.correctly_identified}")


if __name__ == "__main__":
    # Run examples
    print("\nFormFlow AI - Evaluation Framework Examples\n")

    # Example 1: Single form evaluation
    single_result = example_single_form_evaluation()

    # Example 2: Dataset evaluation (if ground truth files exist)
    dataset_result = example_dataset_evaluation()

    # Example 3: Custom similarity thresholds
    example_custom_similarity()

    print("\n" + "="*70)
    print("Examples completed!")
    print("="*70)
